---
title: "testing"
author: "Michael Huang"
date: "2023-03-25"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
# set the path to your Python executable
#install.packages("reticulate")
library(reticulate)
use_python("/Users/michaelhuang/Library/r-miniconda-arm64/envs/r-reticulate/bin/python")

```

```{r}
# set the path to your TensorFlow conda environment
use_condaenv("/Users/michaelhuang/Library/r-miniconda-arm64/envs/r-reticulate")
```

```{r message=FALSE, warning=FALSE}
tensorflow <- import("tensorflow")
library(tensorflow)
install_tensorflow( version = "latest", upgrade = TRUE)
```

```{r}
library(keras)
library(tfdatasets)
```

```{r}
batch_size <- 32
seed <- 42

raw_train_ds <- text_dataset_from_directory(
  '/Users/michaelhuang/Dropbox/University of Chicago/2023 Spring/DIGS 30032/Final project/Urdu News Authorship Classifier/urdu_news/train',
  batch_size = batch_size,
  validation_split = 0.2,
  subset = 'training',
  seed = seed
)
```
```{r}
raw_val_ds <- text_dataset_from_directory(
  '/Users/michaelhuang/Dropbox/University of Chicago/2023 Spring/DIGS 30032/Final project/Urdu News Authorship Classifier/urdu_news/train',
  batch_size = batch_size,
  validation_split = 0.2,
  subset = 'validation',
  seed = seed
)
```

```{r}
raw_test_ds <- text_dataset_from_directory(
  '/Users/michaelhuang/Dropbox/University of Chicago/2023 Spring/DIGS 30032/Final project/Urdu News Authorship Classifier/urdu_news/train',
  batch_size = batch_size
)
```

```{r}
max_features <- 10000
sequence_length <- 250

vectorize_layer <- layer_text_vectorization(
  #standardize = custom_standardization,
  max_tokens = max_features,
  output_mode = "int",
  output_sequence_length = sequence_length
)
```

```{r}
# Make a text-only dataset (without labels), then call adapt
train_text <- raw_train_ds %>%
  dataset_map(function(text, label) text)
vectorize_layer %>% adapt(train_text)
```

```{r}
vectorize_text <- function(text, label) {
  text <- tf$expand_dims(text, -1L)
  list(vectorize_layer(text), label)
}
```

```{r}
train_ds <- raw_train_ds %>% dataset_map(vectorize_text)
val_ds <- raw_val_ds %>% dataset_map(vectorize_text)
test_ds <- raw_test_ds %>% dataset_map(vectorize_text)
```

```{r}
AUTOTUNE <- tf$data$AUTOTUNE

train_ds <- train_ds %>%
  dataset_cache() %>%
  dataset_prefetch(buffer_size = AUTOTUNE)
val_ds <- val_ds %>%
  dataset_cache() %>%
  dataset_prefetch(buffer_size = AUTOTUNE)
test_ds <- test_ds %>%
  dataset_cache() %>%
  dataset_prefetch(buffer_size = AUTOTUNE)
```

```{r}
embedding_dim <- 16
```

```{r}
model <- keras_model_sequential() %>%
  layer_embedding(max_features + 1, embedding_dim) %>%
  layer_dropout(0.2) %>%
  layer_global_average_pooling_1d() %>%
  layer_dropout(0.2) %>%
  layer_dense(1)

summary(model)
```
```{r}
model %>% compile(
  loss = loss_binary_crossentropy(from_logits = TRUE),
  optimizer = 'adam',
  metrics = metric_binary_accuracy(threshold = 0)
)
```

```{r}
model %>% evaluate(test_ds)
```

```{r}
export_model <- keras_model_sequential() %>%
  vectorize_layer() %>%
  model() %>%
  layer_activation(activation = "sigmoid")

export_model %>% compile(
  loss = loss_binary_crossentropy(from_logits = FALSE),
  optimizer = "adam",
  metrics = 'accuracy'
)

# Test it with `raw_test_ds`, which yields raw strings
export_model %>% evaluate(raw_test_ds)
```

